/*
 * This class is used to demonstrate the neural network for given inputs
 * http://www.pirobot.org/blog/0007/
 * http://www4.rgu.ac.uk/files/chapter3%20-%20bp.pdf
 */
public class QueryNeurons {
	
	public static void run() {
		
		double[][] trainingData = {
				{100, 100, 100, 100, 100},
				{50, 50, 50, 50, 50},
				{25, 25, 25, 25, 25},
				{10, 10, 10, 10, 10},
				{100, 100, 100, 100, 100},
				{100, 100, 100, 100, 100}
		};
		
		double[][][] weights = {
				{
					{0.9161413972215784, 0.8562855103992256, 0.5785812953416961, 0.03841316779152698, 0.24677804405331621, 0.4111794911182013, 0.0},
					{0.4825017154865621, 0.10665932959877152, 0.6306036695541347, 0.7143241683644792, 0.8470958144070703, 0.35603628497139617, 0.0},
					{0.01795135920668814, 0.8087415590979318, 0.4106996180802997, 0.2534164892354642, 0.4930172308737656, 0.6115752281577556, 0.0},
					{0.5910221054083171, 0.8006423412711948, 0.7759561666620602, 0.17043628794754478, 0.05557738660359598, 0.17269777662885177, 0.0},
					{0.08675428751760599, 0.3418208000581169, 0.8701307440637033, 0.7300007179265178, 0.3782100877296968, 0.3371320153766346, 0.0},
					{0.613164708094664, 0.9073682978136354, 0.9797609540843447, 0.029516368675914242, 0.06329304182278783, 0.27961153211298306, 0.0},
					{0.37268282617210857, 0.3092046950021749, 0.4569879250374186, 0.29487208033665335, 0.15728839924731983, 0.06305845838954871, 0.0},
					{0.8605205723369278, 0.7975901655129157, 0.281955458260039, 0.5453570638093287, 0.9067240847585746, 0.4902717260827346, 0.0}
				},
				{
					{0.24047749399620819, 0.98752851413161, 0.9775953951228274, 0.8740580922257342, 0.2551165341625243, 0.3293126836486489, 0.9609087584012481},
					{0.08650352977927418, 0.4579198808040124, 0.8507979184023102, 0.8620600886212149, 0.26720936818918434, 0.6512570840005318, 0.28431656715757275},
					{0.8592381782054992, 0.7300260394540069, 0.5637548775922763, 0.7476645814195974, 0.9175975388914605, 0.2797672645971766, 0.41138378341476345},
					{0.4393539709480398, 0.6611206996907122, 0.4628194648785068, 0.182738077549429, 0.5506128366342531, 0.018144474113700824, 0.39091714145474804},
					{0.30132038689259105, 0.4190983729218622, 0.1796381442673186, 0.1379566371586156, 0.24214506492859028, 0.033653949847196156, 0.2965644942661879},
					{0.14844541343635, 0.25097070099570873, 0.6256251197689132, 0.9491428146565055, 0.6370511940451036, 0.4839171615900857, 0.9070815560250127},
					{0.5143840823911145, 0.7412870454156248, 0.6846237367267802, 0.6564990763493265, 0.2150594008044837, 0.9071699228969188, 0.3580034919999743},
					{0.5974588695356218, 0.6096926488853185, 0.625311355541829, 0.17443231844059848, 8.213012718000234E-4, 0.8033362214664578, 0.6943295214168738}
				},
				{
					{0.5105661191125431, 0.5221496752240199, 0.9844950406532381, 0.2264138507151354, 1.040134807577102, 0.0676504005913688, 0.6812210373049504},
					{0.822755300618402, 0.626564044123556, 0.1320205346939065, 0.5508323499182813, 0.34118447465024904, 0.6295081398031701, 0.8161477175549761},
					{0.3239536630325915, 0.2056725885919973, 0.20348478345321955, 0.8531410685673334, 0.4801063120684687, 0.4714246096102402, 0.7739717018530092},
					{1.0167601630029075, 0.32863025812383706, 0.9246334501019551, 1.0177535088448872, 0.22150880746724358, 0.7217231069827967, 0.045898574820348315},
					{0.8390131107292771, 0.5041793767360138, 0.5167919635746772, 0.8692456577955625, 0.9310636337452066, 0.3430647207997965, 0.14162544405139751},
					{0.6184693586346809, 0.382901441326783, 0.4514663773956477, 0.4992173529980663, 0.740369982158359, 0.9812636950926085, 0.21264640194330642},
					{0.7584081215939289, 0.2131932340998638, 0.465497747031577, 0.9098443340342852, 0.577276183368786, 0.4302638839755577, 0.4083846291925889},
					{0.03162922065216467, 0.7931849813372842, 0.9446655838100293, 0.23426929026876586, 0.06052071268395787, 0.08664302945374747, 0.8542690815020044}
				},
				{
					{-3.011427973404444, -2.3547996715455497, -2.3633959569796072, -2.325344317565449, -2.4342211064191157, -2.8945683800242437, -2.7342003626532385},
					{-1.294498316068856, -1.5061878086216263, -1.3170907067498177, -1.3347096426114846, -1.7120323436482088, -0.8532646618658705, -0.8355047095406262},
					{-1.263170807655669, -1.640842373754845, -1.7385167475773087, -0.8591167041898532, -1.3418507531944377, -0.9840816056673711, -1.0309146721692188},
					{0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0},
					{0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0},
					{0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0},
					{0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0},
					{0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0}
				}
			};
		
		boolean[][] active = { // last column is bias!
				{ true, true, true, true, true, true, false, false }, // first one should always be true
				{ true, true, true, true, true, true, true, true },
				{ true, true, true, true, true, true, true, true },
				{ true, true, true, true, true, true, true, true },
				{ true, true, true, false, false, false, false, false } // last one should only have three true
			};
		
		double[][] outputs = new double[active.length][active[0].length]; // allows storage of past calcuations
		int i, j, k, epoch;
		
		
		for (i = 0; i < trainingData.length; i++) {
			trainingData[i][0] = capper( ( (trainingData[i][0] < 0) ? 100 : trainingData[i][0]) / 100.0, 1.0, 0.0 ) ;
			trainingData[i][1] = capper( ( (trainingData[i][1] < 0) ? 100 : trainingData[i][1]) / 100.0, 1.0, 0.0 ) ;
			trainingData[i][2] = capper( ( (trainingData[i][2] < 0) ? 100 : trainingData[i][2]) / 100.0, 1.0, 0.0 ) ;
			trainingData[i][3] = capper( ( (trainingData[i][3] < 0) ? 100 : trainingData[i][3]) / 100.0, 1.0, 0.0 ) ;
			trainingData[i][4] = capper( ( (trainingData[i][4] < 0) ? 100 : trainingData[i][4]) / 100.0, 1.0, 0.0 ) ;
		}
		
		// -------------------------------------------------------
		// |                     BEGIN EPOCH                     |
		// -------------------------------------------------------
		
		for (epoch = 0; epoch < trainingData.length; epoch++) {
			
			outputs[0][0] = trainingData[epoch % trainingData.length][0];
			outputs[0][1] = trainingData[epoch % trainingData.length][1];
			outputs[0][2] = trainingData[epoch % trainingData.length][2];
			outputs[0][3] = trainingData[epoch % trainingData.length][3];
			outputs[0][4] = trainingData[epoch % trainingData.length][4];

			for (i = 0; i < outputs.length; i++) outputs[i][5] = 1.0; // setup biases
			
			// -------------------------------------------------------
			// |                    QUERY NEURONS                    |
			// -------------------------------------------------------
			
			// looping through the layers
			for ( i = 0; i < weights.length; i++ ) {
				
				// loop through rows of output
				for ( j = 0; j < weights[0].length; j++ ) {
					if ( active[i+1][j] ) {
						for ( k = 0; k < weights[0][0].length; k++ ) if ( active[i][k] ) outputs[i+1][j] += outputs[i][k] * weights[i][j][k]; // sum rows
						
						outputs[i+1][j] = sigmoid( outputs[i+1][j] , 0 ); // perform threshold on sums
					}
				}
			}
			
			// scaling outputs of network - for actual use of neural network
			outputs[i][0] = capper( outputs[i][0] * 6 - 3 , -3.0 , 3.0 ); // [-3,3]
			outputs[i][1] = capper( outputs[i][1] * 100 , 0.0 , 100.0 );  // [0:100]
			outputs[i][2] = capper( outputs[i][2] * 100 , 0.0 , 100.0 );  // [0:100]
			
			System.out.println(outputs[i][0] + " " + outputs[i][1] + " " + outputs[i][2]);
		}
		
	}
	
	public static double sigmoid( double x , double thresh ) { return x; /*1.0 / ( 1.0 + Math.exp(thresh-x) );*/ }
	public static double capper(double value, double max, double min) {	return value; /*(value > max) ? max : (value < min) ? min : value ;*/ }
	public static void main(String[] vars) { run(); }
}
